{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import numpy as np\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import pickle, csv\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils import *\n",
    "from model import UNet3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check patched data\n",
    "# Need to fix label values\n",
    "\n",
    "mr_patch= np.load('/Users/nikhil/code/git_repos/skull_seg/test_output/sub04/0.npy')\n",
    "labels = np.unique(mr_patch[:,:,:,1])\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FLAGS\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer(\"epoch\", 1, \"Epoch to train [4]\")\n",
    "flags.DEFINE_string(\"train_patch_dir\", \"/Users/nikhil/code/git_repos/skull_seg/test_output\", \"Directory of the training data [patches]\")\n",
    "flags.DEFINE_bool(\"split_train\", False, \"Whether to split the train data into train and val [False]\")\n",
    "flags.DEFINE_string(\"train_data_dir\", \"/Users/nikhil/code/git_repos/skull_seg/test_input/\", \"Directory of the train data [../BraTS17TrainingData]\")\n",
    "flags.DEFINE_string(\"deploy_data_dir\", \"/Users/nikhil/code/git_repos/skull_seg/test_input/\", \"Directory of the test data [../BraTS17ValidationData]\")\n",
    "flags.DEFINE_string(\"deploy_output_dir\", \"/Users/nikhil/code/git_repos/skull_seg/valid_input/\", \"Directory name of the output data [output]\")\n",
    "flags.DEFINE_string(\"train_csv\", \"../BraTS17TrainingData/survival_data.csv\", \"CSV path of the training data\")\n",
    "flags.DEFINE_string(\"deploy_csv\", \"../BraTS17ValidationData/survival_evaluation.csv\", \"CSV path of the validation data\")\n",
    "flags.DEFINE_integer(\"batch_size\", 10, \"Batch size [1]\")\n",
    "flags.DEFINE_integer(\"seg_features_root\", 8, \"Number of features in the first filter in the seg net [48]\")\n",
    "flags.DEFINE_integer(\"survival_features\", 8, \"Number of features in the survival net [16]\")\n",
    "flags.DEFINE_integer(\"conv_size\", 3, \"Convolution kernel size in encoding and decoding paths [3]\")\n",
    "flags.DEFINE_integer(\"layers\", 3, \"Encoding and deconding layers [3]\")\n",
    "flags.DEFINE_string(\"loss_type\", \"cross_entropy\", \"Loss type in the model [cross_entropy]\")\n",
    "flags.DEFINE_float(\"dropout\", 0.5, \"Drop out ratio [0.5]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoint\", \"Directory name to save the checkpoints [checkpoint]\")\n",
    "flags.DEFINE_string(\"log_dir\", \"logs\", \"Directory name to save logs [logs]\")\n",
    "flags.DEFINE_boolean(\"train\", True, \"True for training, False for deploying [False]\")\n",
    "flags.DEFINE_boolean(\"run_seg\", True, \"True if run segmentation [True]\")\n",
    "flags.DEFINE_boolean(\"run_survival\", False, \"True if run survival prediction [True]\")\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pp = pprint.PrettyPrinter()\n",
    "# pp.pprint(flags.FLAGS.__flags)\n",
    "\n",
    "\n",
    "# Train\n",
    "all_train_paths = []\n",
    "# for dirpath, dirnames, files in os.walk(FLAGS.train_data_dir):\n",
    "#     if os.path.basename(dirpath)[0:7] == 'Brats17':\n",
    "#         all_train_paths.append(dirpath)\n",
    "        \n",
    "subject_dirs = next(os.walk(FLAGS.train_data_dir))[1]\n",
    "for d in subject_dirs: \n",
    "    all_train_paths.append(os.path.join(FLAGS.train_data_dir,d))\n",
    "\n",
    "if FLAGS.split_train:\n",
    "    if os.path.exists(os.path.join(FLAGS.train_patch_dir, 'files.log')):\n",
    "        with open(os.path.join(FLAGS.train_patch_dir, 'files.log'), 'r') as f:\n",
    "            training_paths, testing_paths = pickle.load(f)\n",
    "    else:\n",
    "        all_paths = [os.path.join(FLAGS.train_patch_dir, p) for p in sorted(os.listdir(FLAGS.train_data_dir))]\n",
    "        np.random.shuffle(all_paths)\n",
    "        n_training = int(len(all_paths) * 4 / 5)\n",
    "        training_paths = all_paths[:n_training]\n",
    "        testing_paths = all_paths[n_training:]\n",
    "        # Save the training paths and testing paths\n",
    "        with open(os.path.join(FLAGS.train_data_dir, 'files.log'), 'w') as f:\n",
    "            pickle.dump([training_paths, testing_paths], f)\n",
    "\n",
    "    training_ids = [os.path.basename(i) for i in training_paths]\n",
    "    testing_ids = [os.path.basename(i) for i in testing_paths]\n",
    "\n",
    "    training_survival_data = {}\n",
    "    testing_survival_data = {}\n",
    "    with open(FLAGS.train_csv, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            if row[0] in training_ids:\n",
    "                training_survival_data[row[0]] = (row[1], row[2])\n",
    "            elif row[0] in testing_ids:\n",
    "                testing_survival_data[row[0]] = (row[1], row[2])\n",
    "\n",
    "    training_survival_paths = [p for p in all_train_paths if os.path.basename(p) in training_survival_data.keys()]\n",
    "    testing_survival_paths = [p for p in all_train_paths if os.path.basename(p) in testing_survival_data.keys()]\n",
    "else:\n",
    "    training_paths = [os.path.join(FLAGS.train_patch_dir, name) for name in os.listdir(FLAGS.train_patch_dir)\n",
    "                      if '.log' not in name]\n",
    "    testing_paths = None\n",
    "\n",
    "    training_ids = [os.path.basename(i) for i in training_paths]\n",
    "    training_survival_paths = []\n",
    "    testing_survival_paths = None\n",
    "    training_survival_data = {}\n",
    "    testing_survival_data = None\n",
    "\n",
    "#     with open(FLAGS.train_csv, 'r') as csvfile:\n",
    "#         reader = csv.reader(csvfile)\n",
    "#         for row in reader:\n",
    "#             if row[0] in training_ids:\n",
    "#                 training_survival_data[row[0]] = (row[1], row[2])\n",
    "#     training_survival_paths = [p for p in all_train_paths if os.path.basename(p) in training_survival_data.keys()]\n",
    "\n",
    "if not os.path.exists(FLAGS.checkpoint_dir):\n",
    "    os.makedirs(FLAGS.checkpoint_dir)\n",
    "\n",
    "if not os.path.exists(FLAGS.log_dir):\n",
    "    os.makedirs(FLAGS.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sub04', 'sub05']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "encoding0/w1:0 (float32_ref 3x3x3x1x8) [216, bytes: 864]\n",
      "encoding0/b1:0 (float32_ref 8) [8, bytes: 32]\n",
      "encoding0/bn1/beta:0 (float32_ref 8) [8, bytes: 32]\n",
      "encoding0/w2:0 (float32_ref 3x3x3x8x8) [1728, bytes: 6912]\n",
      "encoding0/b2:0 (float32_ref 8) [8, bytes: 32]\n",
      "encoding0/bn2/beta:0 (float32_ref 8) [8, bytes: 32]\n",
      "encoding1/w1:0 (float32_ref 3x3x3x8x16) [3456, bytes: 13824]\n",
      "encoding1/b1:0 (float32_ref 16) [16, bytes: 64]\n",
      "encoding1/bn1/beta:0 (float32_ref 16) [16, bytes: 64]\n",
      "encoding1/w2:0 (float32_ref 3x3x3x16x16) [6912, bytes: 27648]\n",
      "encoding1/b2:0 (float32_ref 16) [16, bytes: 64]\n",
      "encoding1/bn2/beta:0 (float32_ref 16) [16, bytes: 64]\n",
      "encoding2/w1:0 (float32_ref 3x3x3x16x32) [13824, bytes: 55296]\n",
      "encoding2/b1:0 (float32_ref 32) [32, bytes: 128]\n",
      "encoding2/bn1/beta:0 (float32_ref 32) [32, bytes: 128]\n",
      "encoding2/w2:0 (float32_ref 3x3x3x32x32) [27648, bytes: 110592]\n",
      "encoding2/b2:0 (float32_ref 32) [32, bytes: 128]\n",
      "encoding2/bn2/beta:0 (float32_ref 32) [32, bytes: 128]\n",
      "bottom/w1:0 (float32_ref 3x3x3x32x64) [55296, bytes: 221184]\n",
      "bottom/b1:0 (float32_ref 64) [64, bytes: 256]\n",
      "bottom/bn1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "bottom/w2:0 (float32_ref 3x3x3x64x64) [110592, bytes: 442368]\n",
      "bottom/b2:0 (float32_ref 64) [64, bytes: 256]\n",
      "bottom/bn2/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "decoding2/w:0 (float32_ref 2x2x2x32x64) [16384, bytes: 65536]\n",
      "decoding2/bn/beta:0 (float32_ref 32) [32, bytes: 128]\n",
      "decoding2/w1:0 (float32_ref 3x3x3x64x32) [55296, bytes: 221184]\n",
      "decoding2/b1:0 (float32_ref 32) [32, bytes: 128]\n",
      "decoding2/bn1/beta:0 (float32_ref 32) [32, bytes: 128]\n",
      "decoding2/w2:0 (float32_ref 3x3x3x32x32) [27648, bytes: 110592]\n",
      "decoding2/b2:0 (float32_ref 32) [32, bytes: 128]\n",
      "decoding2/bn2/beta:0 (float32_ref 32) [32, bytes: 128]\n",
      "decoding1/w:0 (float32_ref 2x2x2x16x32) [4096, bytes: 16384]\n",
      "decoding1/bn/beta:0 (float32_ref 16) [16, bytes: 64]\n",
      "decoding1/w1:0 (float32_ref 3x3x3x32x16) [13824, bytes: 55296]\n",
      "decoding1/b1:0 (float32_ref 16) [16, bytes: 64]\n",
      "decoding1/bn1/beta:0 (float32_ref 16) [16, bytes: 64]\n",
      "decoding1/w2:0 (float32_ref 3x3x3x16x16) [6912, bytes: 27648]\n",
      "decoding1/b2:0 (float32_ref 16) [16, bytes: 64]\n",
      "decoding1/bn2/beta:0 (float32_ref 16) [16, bytes: 64]\n",
      "decoding0/w:0 (float32_ref 2x2x2x8x16) [1024, bytes: 4096]\n",
      "decoding0/bn/beta:0 (float32_ref 8) [8, bytes: 32]\n",
      "decoding0/w1:0 (float32_ref 3x3x3x16x8) [3456, bytes: 13824]\n",
      "decoding0/b1:0 (float32_ref 8) [8, bytes: 32]\n",
      "decoding0/bn1/beta:0 (float32_ref 8) [8, bytes: 32]\n",
      "decoding0/w2:0 (float32_ref 3x3x3x8x8) [1728, bytes: 6912]\n",
      "decoding0/b2:0 (float32_ref 8) [8, bytes: 32]\n",
      "decoding0/bn2/beta:0 (float32_ref 8) [8, bytes: 32]\n",
      "logits/w:0 (float32_ref 1x1x1x8x4) [32, bytes: 128]\n",
      "logits/b:0 (float32_ref 4) [4, bytes: 16]\n",
      "Total size of variables: 350836\n",
      "Total bytes of variables: 1403344\n"
     ]
    }
   ],
   "source": [
    "# Segmentation net\n",
    "tf.reset_default_graph()\n",
    "if FLAGS.run_seg:\n",
    "    run_config = tf.ConfigProto()\n",
    "    with tf.Session(config=run_config) as sess:\n",
    "        unet = UNet3D(sess, checkpoint_dir=FLAGS.checkpoint_dir, log_dir=FLAGS.log_dir, training_paths=training_paths,\n",
    "                      testing_paths=testing_paths, batch_size=FLAGS.batch_size, layers=FLAGS.layers,\n",
    "                      features_root=FLAGS.seg_features_root, conv_size=FLAGS.conv_size,\n",
    "                      dropout=FLAGS.dropout, loss_type=FLAGS.loss_type)\n",
    "\n",
    "        if FLAGS.train:\n",
    "            model_vars = tf.trainable_variables()\n",
    "            slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
    "\n",
    "            train_config = {}\n",
    "            train_config['epoch'] = FLAGS.epoch\n",
    "\n",
    "            unet.train(train_config)\n",
    "        else:\n",
    "            # Deploy\n",
    "            if not os.path.exists(FLAGS.deploy_output_dir):\n",
    "                os.makedirs(FLAGS.deploy_output_dir)\n",
    "            unet.deploy(FLAGS.deploy_data_dir, FLAGS.deploy_output_dir)\n",
    "\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.accuracy_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
