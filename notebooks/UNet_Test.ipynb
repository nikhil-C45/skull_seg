{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import numpy as np\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import pickle, csv\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils import *\n",
    "from model import UNet3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check patched data\n",
    "# Need to fix label values\n",
    "\n",
    "mr_patch= np.load('/Users/nikhil/code/git_repos/skull_seg/test_output/sub04/0.npy')\n",
    "labels = np.unique(mr_patch[:,:,:,1])\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAGS\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer(\"epoch\", 1, \"Epoch to train [4]\")\n",
    "flags.DEFINE_string(\"train_patch_dir\", \"/Users/nikhil/code/git_repos/skull_seg/test_output\", \"Directory of the training data [patches]\")\n",
    "flags.DEFINE_bool(\"split_train\", False, \"Whether to split the train data into train and val [False]\")\n",
    "flags.DEFINE_string(\"train_data_dir\", \"/Users/nikhil/code/git_repos/skull_seg/test_input/\", \"Directory of the train data [../BraTS17TrainingData]\")\n",
    "flags.DEFINE_string(\"deploy_data_dir\", \"/Users/nikhil/code/git_repos/skull_seg/test_input/\", \"Directory of the test data [../BraTS17ValidationData]\")\n",
    "flags.DEFINE_string(\"deploy_output_dir\", \"/Users/nikhil/code/git_repos/skull_seg/valid_output/\", \"Directory name of the output data [output]\")\n",
    "flags.DEFINE_string(\"train_csv\", \"../BraTS17TrainingData/survival_data.csv\", \"CSV path of the training data\")\n",
    "flags.DEFINE_string(\"deploy_csv\", \"../BraTS17ValidationData/survival_evaluation.csv\", \"CSV path of the validation data\")\n",
    "flags.DEFINE_integer(\"batch_size\", 10, \"Batch size [1]\")\n",
    "flags.DEFINE_integer(\"seg_features_root\", 8, \"Number of features in the first filter in the seg net [48]\")\n",
    "flags.DEFINE_integer(\"survival_features\", 8, \"Number of features in the survival net [16]\")\n",
    "flags.DEFINE_integer(\"conv_size\", 3, \"Convolution kernel size in encoding and decoding paths [3]\")\n",
    "flags.DEFINE_integer(\"layers\", 1, \"Encoding and deconding layers [3]\")\n",
    "flags.DEFINE_string(\"loss_type\", \"cross_entropy\", \"Loss type in the model [cross_entropy]\")\n",
    "flags.DEFINE_float(\"dropout\", 0.5, \"Drop out ratio [0.5]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoint\", \"Directory name to save the checkpoints [checkpoint]\")\n",
    "flags.DEFINE_string(\"log_dir\", \"logs\", \"Directory name to save logs [logs]\")\n",
    "flags.DEFINE_boolean(\"train\", True, \"True for training, False for deploying [False]\")\n",
    "flags.DEFINE_boolean(\"run_seg\", True, \"True if run segmentation [True]\")\n",
    "flags.DEFINE_boolean(\"run_survival\", False, \"True if run survival prediction [True]\")\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel') # Needed for jupyter to work \n",
    "FLAGS = flags.FLAGS #flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp = pprint.PrettyPrinter()\n",
    "# pp.pprint(flags.FLAGS.__flags)\n",
    "\n",
    "# Train\n",
    "all_train_paths = []\n",
    "# for dirpath, dirnames, files in os.walk(FLAGS.train_data_dir):\n",
    "#     if os.path.basename(dirpath)[0:7] == 'Brats17':\n",
    "#         all_train_paths.append(dirpath)\n",
    "\n",
    "subject_dirs = next(os.walk(FLAGS.train_data_dir))[1]\n",
    "for d in subject_dirs: \n",
    "    all_train_paths.append(os.path.join(FLAGS.train_data_dir,d))\n",
    "\n",
    "train_patch_dir = '/Users/nikhil/code/git_repos/skull_seg/test_output/'\n",
    "training_paths = [os.path.join(FLAGS.train_patch_dir, name) for name in os.listdir(FLAGS.train_patch_dir)\n",
    "                  if '.DS' not in name]\n",
    "testing_paths = None\n",
    "\n",
    "training_ids = [os.path.basename(i) for i in training_paths]\n",
    "training_survival_paths = []\n",
    "testing_survival_paths = None\n",
    "training_survival_data = {}\n",
    "testing_survival_data = None\n",
    "\n",
    "if not os.path.exists(FLAGS.checkpoint_dir):\n",
    "    os.makedirs(FLAGS.checkpoint_dir)\n",
    "\n",
    "if not os.path.exists(FLAGS.log_dir):\n",
    "    os.makedirs(FLAGS.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-31 16:47:55.906273\n",
      "encoding...\n",
      "encoding0\n",
      "bottom\n",
      "decoding...\n",
      "decoding_conv0\n",
      "WARNING:tensorflow:From ../model.py:185: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "encoding0/w1:0 (float32_ref 3x3x3x1x8) [216, bytes: 864]\n",
      "encoding0/b1:0 (float32_ref 8) [8, bytes: 32]\n",
      "bottom/w1:0 (float32_ref 3x3x3x8x16) [3456, bytes: 13824]\n",
      "bottom/b1:0 (float32_ref 16) [16, bytes: 64]\n",
      "decoding_dconv0/w:0 (float32_ref 2x2x2x8x16) [1024, bytes: 4096]\n",
      "decoding_conv0/w1:0 (float32_ref 3x3x3x8x8) [1728, bytes: 6912]\n",
      "decoding_conv0/b1:0 (float32_ref 8) [8, bytes: 32]\n",
      "logits/w:0 (float32_ref 1x1x1x8x2) [16, bytes: 64]\n",
      "logits/b:0 (float32_ref 2) [2, bytes: 8]\n",
      "Total size of variables: 6474\n",
      "Total bytes of variables: 25896\n",
      "train def\n",
      "channels and classes: 1 2\n",
      "train order: 0\n",
      "train order: 1\n",
      "train order: 2\n",
      "train order: 3\n",
      "train order: 4\n",
      "train order: 5\n",
      "train order: 6\n",
      "train order: 7\n",
      "train order: 8\n",
      "train order: 9\n",
      "train order: 10\n",
      "train order: 11\n",
      "train order: 12\n",
      "train order: 13\n",
      "train order: 14\n",
      "train order: 15\n",
      "train order: 16\n",
      "train order: 17\n",
      "train order: 18\n",
      "train order: 19\n",
      "train order: 20\n",
      "train order: 21\n",
      "train order: 22\n",
      "train order: 23\n",
      "train order: 24\n",
      "train order: 25\n",
      "train order: 26\n",
      "train order: 27\n",
      "train order: 28\n",
      "train order: 29\n",
      "train order: 30\n",
      "train order: 31\n",
      "train order: 32\n",
      "train order: 33\n",
      "train order: 34\n",
      "train order: 35\n",
      "train order: 36\n",
      "train order: 37\n",
      "train order: 38\n",
      "train order: 39\n",
      "train order: 40\n",
      "train order: 41\n",
      "train order: 42\n",
      "train order: 43\n",
      "train order: 44\n",
      "train order: 45\n",
      "train order: 46\n",
      "train order: 47\n",
      "train order: 48\n",
      "train order: 49\n",
      "train order: 50\n",
      "train order: 51\n",
      "train order: 52\n",
      "train order: 53\n",
      "train order: 54\n",
      "train order: 55\n",
      "train order: 56\n",
      "train order: 57\n",
      "train order: 58\n",
      "train order: 59\n",
      "train order: 60\n",
      "train order: 61\n",
      "train order: 62\n",
      "train order: 63\n",
      "train order: 64\n",
      "train order: 65\n",
      "train order: 66\n",
      "train order: 67\n",
      "train order: 68\n",
      "train order: 69\n",
      "train order: 70\n",
      "train order: 71\n",
      "train order: 72\n",
      "train order: 73\n",
      "train order: 74\n",
      "train order: 75\n",
      "train order: 76\n",
      "train order: 77\n",
      "train order: 78\n",
      "train order: 79\n",
      "Training complete\n",
      "start: 2018-10-31 16:47:55.906273, end: 2018-10-31 16:59:05.528953\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print(start_time)\n",
    "\n",
    "if FLAGS.run_seg:\n",
    "    run_config = tf.ConfigProto()\n",
    "    with tf.Session(config=run_config) as sess:\n",
    "    #with tf.Session() as sess:\n",
    "    \n",
    "        unet = UNet3D(sess, checkpoint_dir=FLAGS.checkpoint_dir, log_dir=FLAGS.log_dir, training_paths=training_paths,\n",
    "                      testing_paths=testing_paths, batch_size=FLAGS.batch_size, layers=FLAGS.layers,\n",
    "                      features_root=FLAGS.seg_features_root, conv_size=FLAGS.conv_size,\n",
    "                      dropout=FLAGS.dropout, loss_type=FLAGS.loss_type)\n",
    "\n",
    "        if FLAGS.train:\n",
    "            model_vars = tf.trainable_variables()\n",
    "            slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
    "\n",
    "            train_config = {}\n",
    "            train_config['epoch'] = FLAGS.epoch\n",
    "\n",
    "            unet.train(train_config)\n",
    "        \n",
    "        print('Training complete')\n",
    "#         print('\\nTesting trained model with actual MR image')\n",
    "#         if not os.path.exists(FLAGS.deploy_output_dir):\n",
    "#             os.makedirs(FLAGS.deploy_output_dir)\n",
    "\n",
    "#         unet.deploy(FLAGS.deploy_data_dir, FLAGS.deploy_output_dir)\n",
    "#         print('Test complete. Output at {}'.format(FLAGS.deploy_output_dir))\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print('start: {}, end: {}'.format(start_time, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
